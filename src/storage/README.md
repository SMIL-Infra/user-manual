# 介绍

我们部署了[Ceph](https://ceph.io/)作为我们的存储解决方案。使用该系统主要有如下好处：
* 分布式，所有文件均分布在各个存储节点上，这能带来以下优势：
  * 高性能：读写操作由多个节点同时进行，吞吐量比本地存储更高。
  * 高可用：无单点故障，任意节点宕机均不会导致数据不可用。
* 使用便捷，在所有服务器上都能访问你的所有文件。
* 节省空间，数据集等无需再手动在各台服务器之间拷贝。
* 在所有服务器间同步开发环境，避免反复安装。（有待测试）

## Cephfs

Cephfs是基于Ceph存储技术构建的文件系统，也是大家使用存储系统的主要接口。挂载于每台服务器的`/mnt/cephfs`目录下。

### 状态

目前Cephfs处于测试中，并将逐步推广至所有大学城校区的服务器，以下服务器目前已接入：gpu013

在使用Cephfs前，请确保已完成[统一认证迁移](../auth/migration.md)，否则在使用过程中将遇到权限问题。

### 使用规划

存储布局规划：

* 主存储：
  * 路径：`/mnt/cephfs/`
  * 存储布局：HDD 2副本（计划提升至3副本）
  * 计划用途：一般用途，如实验数据，开发环境，中间结果，checkpoint等
  * 性能：连续读写较快，随机读写较慢

* 高性能数据集：
  * 路径：`/mnt/cephfs/mixed/`
  * 存储布局：SSD 1副本 + HDD 2副本（计划提升至3副本）
  * 计划用途：需要高随机读取性能的数据集
  * 性能：读取非常快，连续写入较快，随机写入较慢

* 冷数据（规划中）：
  * 路径：`/mnt/cephfs/cold/`
  * 存储布局：HDD Erasure Coding 4+2
  * 计划用途：之前实验用到的数据集，实验数据，checkpoint等较少访问的数据
  * 性能：连续读写较快，随机读取较慢，随机写入非常慢

* 高性能（不常用）：
  * 路径：`/mnt/cephfs/ssd/`
  * 存储布局：SSD 2副本
  * 计划用途：数据处理所需临时占用
  * 性能：非常快

每个文件实际使用的存储布局只取决于它创建时所在的路径。创建后可以移动到任何`/mnt/cephfs/`下的目录中而不影响其布局。因此，当需要转换文件的存储布局时，请使用`cp`命令而不是`mv`。

使用权限：

* `/mnt/cephfs/home/<用户名>`: 相应用户拥有所有权，可自行决定存放的数据
  * 限额：512G
* `/mnt/cephfs/cold/home/<用户名>`（规划中）: 相应用户拥有所有权，可自行决定存放的数据
  * 限额：1T
* `/mnt/cephfs/dataset/`: 用户可自行判断，在其中创建文件夹，存储可能被多个用户重用的数据集
* `/mnt/cephfs/mixed/dataset/`: 请联系管理员创建文件夹并设置限额，存储需高随机读取性能，或非常常用的数据集
* `/mnt/cephfs/ssd/`: 请联系管理员使用
